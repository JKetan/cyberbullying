{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vffL9zMsDUOo"
      },
      "outputs": [],
      "source": [
        "# Ignore  the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer  # lemmatizer from WordNet\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "import re # regex\n",
        "\n",
        "#model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score \n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#preprocessing scikit\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#classifiaction.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#keras\n",
        "import keras\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "#gensim w2v\n",
        "#word2vec\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIrqQdukE49b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/"
      ],
      "metadata": {
        "id": "Brl6gzfOp2l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwFmBRsPFRpV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('cyberbullying_tweets.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx13qQ7CHQL6"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={\"tweet_text\": \"tweet\", \"cyberbullying_type\": \"label\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['label'].value_counts()\n",
        "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, \n",
        "        colors = sns.color_palette(\"pastel\")[4:])\n",
        "plt.text(x = -0.40, y = 0, s = 'Total number of Tweets:')\n",
        "plt.text(x = -0.15, y = -0.1, s = format(df.shape[0]))\n",
        "plt.title('Pie chart for : Tweets in the Dataset', fontsize = 16);"
      ],
      "metadata": {
        "id": "qKNUK6ZngD1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0debaxcHIPj"
      },
      "outputs": [],
      "source": [
        "print(df['tweet'].isnull().sum())\n",
        "print(df['label'].isnull().sum())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0wfuJOMH8OH"
      },
      "outputs": [],
      "source": [
        "for tweet in df['tweet'][:5]:\n",
        "    print(tweet+'\\n'+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGdpiaANJXSY"
      },
      "outputs": [],
      "source": [
        "print(df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbAJnNSyJH02"
      },
      "outputs": [],
      "source": [
        "def rename_label(label):\n",
        "    if(label == 'not_cyberbullying'):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_olFHs8jJjPh"
      },
      "outputs": [],
      "source": [
        "df['label']=df['label'].apply(rename_label)\n",
        "print(df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (7,7))\n",
        "sorted_counts = df['label'].value_counts()\n",
        "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90, counterclock = False, wedgeprops = {'width' : 0.6},\n",
        "       autopct='%1.1f%%', pctdistance = 0.7, textprops = {'color': 'black', 'fontsize' : 15}, \n",
        "        colors = sns.color_palette(\"pastel\")[4:])\n",
        "plt.text(x = -0.40, y = 0, s = 'Total number of Tweets:')\n",
        "plt.text(x = -0.15, y = -0.1, s = format(df.shape[0]))\n",
        "plt.title('Pie chart for : Tweets in the Dataset', fontsize = 16);"
      ],
      "metadata": {
        "id": "C77x21mpoH4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyimcssILPPd"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yh8wmaNj69C"
      },
      "outputs": [],
      "source": [
        "def cleanup(tweet):  \n",
        "    \n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "    tweet = re.sub(\"[^a-zA-Z]\",\" \",tweet)\n",
        "    word_tokens= tweet.lower().split()\n",
        "\n",
        "    \n",
        "    # Remove stopwords\n",
        "    le=WordNetLemmatizer()\n",
        "    stop_words= set(stopwords.words(\"english\"))     \n",
        "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "    cleaned=\" \".join(word_tokens)\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgQLkfXCkf3P"
      },
      "outputs": [],
      "source": [
        "# shuffling rows\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCNeoImqlSGb"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences=[]\n",
        "sum=0\n",
        "\n",
        "sumbeforecleaning=0\n",
        "for tweet in df['tweet']:\n",
        "    sumbeforecleaning +=len(tweet)\n",
        "    cleaned_sents=cleanup(tweet.strip())\n",
        "    sum += len(cleaned_sents)\n",
        "    sents=tokenizer.tokenize(cleaned_sents)\n",
        "    for sent in sents:\n",
        "        sentences.append(sent.split())\n",
        "\n",
        "print(sumbeforecleaning)\n",
        "print(sum)\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAchTtDl2lNe"
      },
      "outputs": [],
      "source": [
        "  # trying to print few sentences\n",
        "for te in sentences[:5]:\n",
        "    print(te,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kY2Nt0TowPR"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "w2v_model=gensim.models.Word2Vec(sentences=sentences,size=300,window=10,min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPn-oPQjo570"
      },
      "outputs": [],
      "source": [
        "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8Mj4RGFpIds"
      },
      "outputs": [],
      "source": [
        "# total numberof extracted words.\n",
        "vocab=w2v_model.wv.vocab\n",
        "print(\"The total number of words are : \",len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyvfIL6Pqe4Y"
      },
      "outputs": [],
      "source": [
        "vocab=list(vocab.keys())\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to print few vocab\n",
        "for v in vocab[:5]:\n",
        "    print(v,\"\\n\")"
      ],
      "metadata": {
        "id": "BXqd8DQ_q_43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BH871iAfqizx"
      },
      "outputs": [],
      "source": [
        "word_vec_dict={} #dict of words we are creating for our vocab\n",
        "for word in vocab:\n",
        "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
        "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18mFDOwDqm59"
      },
      "outputs": [],
      "source": [
        "# cleaning\n",
        "df['tweet']=df['tweet'].apply(cleanup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DipC_Bn7q1DJ"
      },
      "outputs": [],
      "source": [
        "#to find the maximum lenght of any document.\n",
        "\n",
        "max=-1\n",
        "for i,rev in enumerate(df['tweet']):\n",
        "    tokens=rev.split()\n",
        "    if(len(tokens)>max):\n",
        "        max=len(tokens)\n",
        "print(max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyrSffp2q_oz"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(df['tweet']) #Updates internal vocabulary based on a list of texts. word_index gives an index to each word\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "print(vocab_size)\n",
        "encd_rev = tok.texts_to_sequences(df['tweet']) #Transforms each text in texts to a sequence of integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44YoTFT5rQrX"
      },
      "outputs": [],
      "source": [
        "max_len = 496\n",
        "vocab_size = len(tok.word_index) + 1  # total no of words\n",
        "embed_dim=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTUf0_8SsqOG"
      },
      "outputs": [],
      "source": [
        "pad_rev= pad_sequences(encd_rev, maxlen=max_len, padding='post')\n",
        "pad_rev.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(tok.word_index.items())[:5])"
      ],
      "metadata": {
        "id": "TR_2LUKs2TGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwDbcGycrJjo"
      },
      "outputs": [],
      "source": [
        "# creating the embedding matrix\n",
        "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
        "for word,i in tok.word_index.items():\n",
        "  embed_vector=word_vec_dict.get(word)\n",
        "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
        "    embed_matrix[i]=embed_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X66s4g8Mraio"
      },
      "outputs": [],
      "source": [
        "# prepare train and val sets first\n",
        "Y=keras.utils.to_categorical(df['label'])  # one hot target as required by NN.\n",
        "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42, stratify = Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rqp9v0IrqxQ"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRKG-ZOB2PW0"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embed_dim,input_length=max_len,embeddings_initializer=Constant(embed_matrix)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LSTM(64, input_shape=(10,1))) \n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(2,activation='softmax')) \n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "epochs=5\n",
        "batch_size=64\n",
        "\n",
        "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_test_bool=np.argmax(y_test,axis=1)\n",
        "y_pred = model.predict(x_test, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test_bool, y_pred_bool))"
      ],
      "metadata": {
        "id": "s4WGfE72y-LT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}