{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKoxE4y28kZB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from sklearn import metrics\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import re\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import WordNetLemmatizer  # lemmatizer from WordNet\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "import gensim\n",
        "\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdOoMj71_BCW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSu4W3pN_Cfj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/cyberbullying_tweets.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8X0o4io_aGx"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={\"tweet_text\": \"tweet\", \"cyberbullying_type\": \"label\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn0sdwF6_c8k"
      },
      "outputs": [],
      "source": [
        "print(df['tweet'].isnull().sum())\n",
        "print(df['label'].isnull().sum())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeskQ32w_g8k"
      },
      "outputs": [],
      "source": [
        "for tweet in df['tweet'][:5]:\n",
        "    print(tweet+'\\n'+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr2uZD_m_lfp"
      },
      "outputs": [],
      "source": [
        "print(df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI2RCo6I_n8Y"
      },
      "outputs": [],
      "source": [
        "def rename_label(label):\n",
        "    if(label == 'not_cyberbullying'):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doXCu3xW_qmB"
      },
      "outputs": [],
      "source": [
        "df['label']=df['label'].apply(rename_label)\n",
        "print(df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SETgof16_uV2"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izvWzXRyAmmT"
      },
      "outputs": [],
      "source": [
        "def cleanup(tweet):  \n",
        "    \n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "    tweet = re.sub(\"[^a-zA-Z]\",\" \",tweet)\n",
        "    word_tokens= tweet.lower().split()\n",
        "    \n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    stop_words= set(stopwords.words(\"english\"))     \n",
        "    word_tokens= [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "    return \" \".join(word_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3DGuxlUCc9M"
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O7TqDyDCiyW"
      },
      "outputs": [],
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences=[]\n",
        "sum=0\n",
        "\n",
        "sumbeforecleaning=0\n",
        "for tweet in df['tweet']:\n",
        "    sumbeforecleaning +=len(tweet)\n",
        "    cleaned_sents=cleanup(tweet.strip())\n",
        "    sum+=len(cleaned_sents)\n",
        "    sents=tokenizer.tokenize(cleaned_sents)\n",
        "    for sent in sents:\n",
        "        sentences.append(sent.split()) # can use word_tokenize also.\n",
        "\n",
        "print(sumbeforecleaning)\n",
        "print(sum)\n",
        "print(len(sentences))  # total no of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9eRhhnxC_t4"
      },
      "outputs": [],
      "source": [
        "for te in sentences[:5]:\n",
        "    print(te,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS3XnXInDX6G"
      },
      "outputs": [],
      "source": [
        "w2v_model=gensim.models.Word2Vec(sentences= sentences,size=300,window=10,min_count=1)\n",
        "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kd3NwXeDnFy"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.get_vector('bitch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rMc9xPKDoIb"
      },
      "outputs": [],
      "source": [
        "vocab=w2v_model.wv.vocab\n",
        "print(\"The total number of words are : \",len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYh17U-vDszC"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.most_similar('hoe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggYyys-HDvOE"
      },
      "outputs": [],
      "source": [
        "w2v_model.wv.similarity('hoe','whore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gY7VjDDDx59"
      },
      "outputs": [],
      "source": [
        "vocab=list(vocab.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Us2FTqGD0Bs"
      },
      "outputs": [],
      "source": [
        "word_vec_dict={}\n",
        "for word in vocab:\n",
        "  word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
        "print(\"The no of key-value pairs : \",len(word_vec_dict))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvSW6pbLEEMg"
      },
      "outputs": [],
      "source": [
        "df['tweet']=df['tweet'].apply(cleanup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JJ3JPoZEEhM"
      },
      "outputs": [],
      "source": [
        "maximum=-1\n",
        "for i,rev in enumerate(df['tweet']):\n",
        "    tokens=rev.split()\n",
        "    if(len(tokens)>maximum):\n",
        "        maximum=len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK7ZgyR_EK33"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(df['tweet'])\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "encd_rev = tok.texts_to_sequences(df['tweet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P37CO_1JEmpI"
      },
      "outputs": [],
      "source": [
        "max_len = 496\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "embed_dim=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5BEod2lLPId"
      },
      "outputs": [],
      "source": [
        "pad_rev_sampling= pad_sequences(x_text, maxlen=max_len, padding='post')\n",
        "pad_rev_sampling.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ZTCwf-EotF"
      },
      "outputs": [],
      "source": [
        "pad_rev= pad_sequences(encd_rev, maxlen=max_len, padding='post')\n",
        "pad_rev.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHVfrwDEFAsY"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(pad_rev,df['label'],test_size=0.20,random_state=42, stratify = df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrkw9cN4FHR-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "logreg = LogisticRegression(max_iter = 5000)\n",
        "logreg.fit(x_train, y_train)\n",
        "y_pred_class = logreg.predict(x_test)\n",
        "print((metrics.accuracy_score(y_test, y_pred_class)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrtFa0dbCykW"
      },
      "outputs": [],
      "source": [
        "print(precision_recall_fscore_support(y_test, y_pred_class, average = 'weighted'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}